# Introduction


Ce projet a pour objectif de mettre en place une solution d'intelligence artificielle g√©n√©rative en auto-h√©bergement. Il s'appuie sur deux composants principaux :

- Ollama : un outil permettant de t√©l√©charger, g√©rer et ex√©cuter localement des mod√®les de langage (LLMs).

- Open WebUI : une interface web moderne et intuitive pour interagir avec ces mod√®les en toute simplicit√©.

Cette plateforme permet √† un utilisateur de poser des questions en langage naturel et d'obtenir des r√©ponses coh√©rentes, contextuelles, et intelligentes, sans d√©pendre de services cloud externes.

## Qu'est-ce qu'un LLM ?

Un LLM (ou mod√®le de langage √† grande √©chelle) est un mod√®le d‚Äôintelligence artificielle entra√Æn√© sur d‚Äô√©normes volumes de texte. Ces mod√®les apprennent les structures, relations, et significations dans le langage humain. Une fois entra√Æn√©, un LLM est capable de :

- Comprendre et g√©n√©rer du texte en langage naturel

- R√©pondre √† des questions, traduire des textes, r√©sumer des documents

- Simuler une conversation naturelle, √©crire du code, etc.

Les mod√®les les plus connus aujourd'hui sont GPT-3, GPT-4, LLaMA (Meta), Mistral, etc. Ces mod√®les contiennent des milliards de param√®tres : ce sont des fonctions math√©matiques ajust√©es pendant l'entra√Ænement pour pr√©dire le mot suivant dans une phrase.

Par exemple :

**Entr√©e** : *"Le ciel est bleu parce que..."*

**Sortie :** *"...la lumi√®re du soleil est diffus√©e par l‚Äôatmosph√®re terrestre."*

## Pourquoi utiliser Ollama et Open WebUI ?

Ollama permet d'ex√©cuter localement les LLMs, sans envoyer vos donn√©es vers un serveur distant.

Open WebUI fournit une interface √©l√©gante pour discuter avec ces mod√®les, comparable √† ChatGPT, mais auto-h√©berg√©e, respectueuse de la vie priv√©e et enti√®rement personnalisable.

Ce projet est donc une alternative locale, libre et extensible aux assistants IA disponibles sur Internet.

## O√π sont stock√©es les donn√©es ?

Ce projet est con√ßu pour **fonctionner enti√®rement en local**, garantissant ainsi **la confidentialit√© totale des donn√©es** :

- **Les mod√®les sont stock√©s sur la machine locale**, dans le dossier `~/.ollama`.
- **Les historiques de conversation sont enregistr√©s dans le volume Docker d'Open WebUI**, accessible uniquement par l'utilisateur du syst√®me.
- Aucune donn√©e n‚Äôest envoy√©e vers des serveurs distants, sauf configuration explicite.

Ainsi, toutes les interactions avec les mod√®les d'IA sont **priv√©es, s√©curis√©es, et sous ton contr√¥le total**.


## Environnement de d√©ploiement ‚Äì Machine Virtuelle

Pour garantir de bonnes performances √† l‚Äô**IA g√©n√©rative auto-h√©berg√©e**, ce projet a √©t√© d√©ploy√© sur une **machine virtuelle (VM)** puissante, optimis√©e pour l‚Äôex√©cution de mod√®les LLM locaux, notamment ceux propos√©s via **Ollama**.

### Sp√©cifications techniques de la VM

| Ressource       | D√©tail                        |
|------------------|-------------------------------|
|  Processeurs    | 16 c≈ìurs                      |
|  M√©moire RAM    | 50 Go                         |
|  Stockage SSD  | 500 Go                        |
|  GPU           | NVIDIA L4 avec 24 Go de VRAM  |
|  Adresse IP     | `159.31.247.122`              |

Cette configuration permet :

-  Le **chargement rapide** et **l‚Äôinf√©rence fluide** de mod√®les de langage volumineux (ex : LLaMA3, Mistral, etc.)
-  Une **installation 100% locale** sans compromis sur les performances
-  Le **support GPU (CUDA)** pour l‚Äôacc√©l√©ration des t√¢ches d‚Äôinf√©rence via la carte NVIDIA L4 (24 Go VRAM), indispensable pour des mod√®les LLM modernes

### üéØ Pourquoi ce choix de VM ?

Le choix de cette VM repose sur plusieurs crit√®res cl√©s pour une plateforme d'IA locale :

- **Compatibilit√© GPU avec Ollama et PyTorch** (acc√©l√©ration CUDA)
- **M√©moire suffisante** pour charger des mod√®les en RAM + VRAM sans swap
- **Stockage cons√©quent** pour accueillir plusieurs mod√®les (~2-10 Go chacun) et les historiques utilisateurs
- **S√©curit√© et isolation** : environnement d√©di√© pour l‚ÄôIA g√©n√©rative, sans interf√©rence avec d'autres services
